{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 1. Build your own convolutional neural network using pytorch"]},{"cell_type":"code","execution_count":14,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-11-20T03:34:28.495835Z","iopub.status.busy":"2024-11-20T03:34:28.495185Z","iopub.status.idle":"2024-11-20T03:34:28.501666Z","shell.execute_reply":"2024-11-20T03:34:28.500875Z","shell.execute_reply.started":"2024-11-20T03:34:28.495800Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import transforms, datasets\n","from torch.utils.data import DataLoader, Dataset\n","from PIL import Image\n","import pandas as pd\n","import os\n","\n","# Device setup\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-11-20T03:34:28.506704Z","iopub.status.busy":"2024-11-20T03:34:28.506429Z","iopub.status.idle":"2024-11-20T03:34:28.753822Z","shell.execute_reply":"2024-11-20T03:34:28.752832Z","shell.execute_reply.started":"2024-11-20T03:34:28.506676Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["DogHeartCNN(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): ResidualBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (se): SEBlock(\n","        (fc1): Linear(in_features=64, out_features=4, bias=True)\n","        (fc2): Linear(in_features=4, out_features=64, bias=True)\n","        (relu): ReLU()\n","        (sigmoid): Sigmoid()\n","      )\n","      (dropout): Dropout2d(p=0.2, inplace=False)\n","    )\n","    (1): ResidualBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (se): SEBlock(\n","        (fc1): Linear(in_features=64, out_features=4, bias=True)\n","        (fc2): Linear(in_features=4, out_features=64, bias=True)\n","        (relu): ReLU()\n","        (sigmoid): Sigmoid()\n","      )\n","      (dropout): Dropout2d(p=0.2, inplace=False)\n","    )\n","    (2): ResidualBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (se): SEBlock(\n","        (fc1): Linear(in_features=64, out_features=4, bias=True)\n","        (fc2): Linear(in_features=4, out_features=64, bias=True)\n","        (relu): ReLU()\n","        (sigmoid): Sigmoid()\n","      )\n","      (dropout): Dropout2d(p=0.2, inplace=False)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): ResidualBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (se): SEBlock(\n","        (fc1): Linear(in_features=128, out_features=8, bias=True)\n","        (fc2): Linear(in_features=8, out_features=128, bias=True)\n","        (relu): ReLU()\n","        (sigmoid): Sigmoid()\n","      )\n","      (dropout): Dropout2d(p=0.3, inplace=False)\n","    )\n","    (1): ResidualBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (se): SEBlock(\n","        (fc1): Linear(in_features=128, out_features=8, bias=True)\n","        (fc2): Linear(in_features=8, out_features=128, bias=True)\n","        (relu): ReLU()\n","        (sigmoid): Sigmoid()\n","      )\n","      (dropout): Dropout2d(p=0.3, inplace=False)\n","    )\n","    (2): ResidualBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (se): SEBlock(\n","        (fc1): Linear(in_features=128, out_features=8, bias=True)\n","        (fc2): Linear(in_features=8, out_features=128, bias=True)\n","        (relu): ReLU()\n","        (sigmoid): Sigmoid()\n","      )\n","      (dropout): Dropout2d(p=0.3, inplace=False)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): ResidualBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (se): SEBlock(\n","        (fc1): Linear(in_features=256, out_features=16, bias=True)\n","        (fc2): Linear(in_features=16, out_features=256, bias=True)\n","        (relu): ReLU()\n","        (sigmoid): Sigmoid()\n","      )\n","      (dropout): Dropout2d(p=0.3, inplace=False)\n","    )\n","    (1): ResidualBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (se): SEBlock(\n","        (fc1): Linear(in_features=256, out_features=16, bias=True)\n","        (fc2): Linear(in_features=16, out_features=256, bias=True)\n","        (relu): ReLU()\n","        (sigmoid): Sigmoid()\n","      )\n","      (dropout): Dropout2d(p=0.3, inplace=False)\n","    )\n","    (2): ResidualBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (se): SEBlock(\n","        (fc1): Linear(in_features=256, out_features=16, bias=True)\n","        (fc2): Linear(in_features=16, out_features=256, bias=True)\n","        (relu): ReLU()\n","        (sigmoid): Sigmoid()\n","      )\n","      (dropout): Dropout2d(p=0.3, inplace=False)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): ResidualBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (se): SEBlock(\n","        (fc1): Linear(in_features=512, out_features=32, bias=True)\n","        (fc2): Linear(in_features=32, out_features=512, bias=True)\n","        (relu): ReLU()\n","        (sigmoid): Sigmoid()\n","      )\n","      (dropout): Dropout2d(p=0.4, inplace=False)\n","    )\n","    (1): ResidualBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (se): SEBlock(\n","        (fc1): Linear(in_features=512, out_features=32, bias=True)\n","        (fc2): Linear(in_features=32, out_features=512, bias=True)\n","        (relu): ReLU()\n","        (sigmoid): Sigmoid()\n","      )\n","      (dropout): Dropout2d(p=0.4, inplace=False)\n","    )\n","    (2): ResidualBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (se): SEBlock(\n","        (fc1): Linear(in_features=512, out_features=32, bias=True)\n","        (fc2): Linear(in_features=32, out_features=512, bias=True)\n","        (relu): ReLU()\n","        (sigmoid): Sigmoid()\n","      )\n","      (dropout): Dropout2d(p=0.4, inplace=False)\n","    )\n","    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","  )\n","  (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (max_pool): AdaptiveMaxPool2d(output_size=(1, 1))\n","  (dropout): Dropout(p=0.4, inplace=False)\n","  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n","  (fc2): Linear(in_features=512, out_features=3, bias=True)\n",")\n"]}],"source":["class SEBlock(nn.Module):\n","    def __init__(self, channels, reduction=16):\n","        super(SEBlock, self).__init__()\n","        self.fc1 = nn.Linear(channels, channels // reduction)\n","        self.fc2 = nn.Linear(channels // reduction, channels)\n","        self.relu = nn.ReLU()\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        batch, channels, _, _ = x.size()\n","        squeeze = x.view(batch, channels, -1).mean(dim=2)  # Global Average Pooling\n","        excitation = self.fc2(self.relu(self.fc1(squeeze)))\n","        excitation = self.sigmoid(excitation).view(batch, channels, 1, 1)\n","        return x * excitation\n","\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride=1, downsample=None, dropout_rate=0.3):\n","        super(ResidualBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","        self.downsample = downsample\n","        self.se = SEBlock(out_channels)\n","        self.dropout = nn.Dropout2d(p=dropout_rate)\n","\n","    def forward(self, x):\n","        residual = x\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out = self.dropout(out)\n","        out = self.se(out)  # Apply SE Block\n","        if self.downsample:\n","            residual = self.downsample(x)\n","        out += residual\n","        return F.relu(out)\n","\n","\n","class DogHeartCNN(nn.Module):\n","    def __init__(self, num_classes=3, dropout_rate=0.4):\n","        super(DogHeartCNN, self).__init__()\n","        # Initial Convolutional Layer\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","\n","        # Residual Blocks with Multi-Scale Extraction\n","        self.layer1 = self._make_layer(64, 64, 3, stride=1, dropout_rate=0.2)\n","        self.layer2 = self._make_layer(64, 128, 3, stride=2, dropout_rate=0.3)\n","        self.layer3 = self._make_layer(128, 256, 3, stride=2, dropout_rate=0.3)\n","        self.layer4 = self._make_layer(256, 512, 3, stride=2, dropout_rate=0.4, multi_scale=True)\n","\n","        # Adaptive Pooling and Fully Connected Layers\n","        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.max_pool = nn.AdaptiveMaxPool2d((1, 1))\n","        self.dropout = nn.Dropout(p=dropout_rate)\n","        self.fc1 = nn.Linear(512 * 2, 512)  # Concatenate Avg & Max Pool features\n","        self.fc2 = nn.Linear(512, num_classes)\n","\n","    def _make_layer(self, in_channels, out_channels, blocks, stride, dropout_rate, multi_scale=False):\n","        downsample = None\n","        if stride != 1 or in_channels != out_channels:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n","                nn.BatchNorm2d(out_channels),\n","            )\n","        layers = [ResidualBlock(in_channels, out_channels, stride, downsample, dropout_rate)]\n","        for _ in range(1, blocks):\n","            layers.append(ResidualBlock(out_channels, out_channels, dropout_rate=dropout_rate))\n","\n","        if multi_scale:\n","            layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=3, dilation=2, padding=2))  # Dilated conv\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        # Initial Convolutional Layer\n","        x = F.relu(self.bn1(self.conv1(x)))\n","        x = self.pool(x)\n","\n","        # Residual Layers\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        # Adaptive Pooling\n","        avg_features = self.avg_pool(x)\n","        max_features = self.max_pool(x)\n","        x = torch.cat([avg_features, max_features], dim=1)  # Concatenate pooled features\n","        x = torch.flatten(x, 1)\n","\n","        # Fully Connected Layers\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","        return x\n","\n","\n","# Initialize the model\n","model = DogHeartCNN(num_classes=3).to('cuda' if torch.cuda.is_available() else 'cpu')\n","print(model)\n"]},{"cell_type":"markdown","metadata":{},"source":["# 2. Train your model using dog heart dataset (you may need to use  Google Colab (or Kaggle) with GPU to train your code) \n","\n","### (1) use torchvision.datasets.ImageFolder for the training dataset\n","### (2) use custom dataloader for test dataset (return image tensor and file name)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-11-20T03:34:28.755519Z","iopub.status.busy":"2024-11-20T03:34:28.755244Z","iopub.status.idle":"2024-11-20T03:34:31.701840Z","shell.execute_reply":"2024-11-20T03:34:31.700871Z","shell.execute_reply.started":"2024-11-20T03:34:28.755492Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Datasets and loaders prepared with updated transformations and test dataset.\n"]}],"source":["import os\n","import torch\n","from torchvision import transforms, datasets\n","from torch.utils.data import DataLoader\n","from PIL import Image\n","\n","# Updated transformations with cropping and normalization\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),  # Resize to 224x224\n","    transforms.RandomHorizontalFlip(p=0.5),  # Random horizontal flip\n","    transforms.ToTensor(),  # Convert to PyTorch tensor\n","    transforms.Normalize(mean=[0.4926, 0.4927, 0.4926], std=[0.2077, 0.2076, 0.2077])  # Normalize\n","])\n","\n","# Load training and validation datasets\n","train_data = datasets.ImageFolder(\n","    '/kaggle/input/d/shraddhabelbase/dogxray/Dog_X_ray/Dog_heart/Dog_heart/Train',\n","    transform\n",")\n","val_data = datasets.ImageFolder(\n","    '/kaggle/input/d/shraddhabelbase/dogxray/Dog_X_ray/Dog_heart/Dog_heart/Valid',\n","    transform\n",")\n","\n","# Custom test dataset for no subfolder structure\n","class TestDataset(torch.utils.data.Dataset):\n","    def __init__(self, root_dir, transform=None):\n","        self.root_dir = root_dir\n","        self.image_files = [f for f in os.listdir(root_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_files)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.root_dir, self.image_files[idx])\n","        image = Image.open(img_path).convert('RGB')\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, self.image_files[idx]\n","\n","# Load the test dataset\n","test_data = TestDataset(\n","    '/kaggle/input/d/shraddhabelbase3/dogxray/Dog_X_ray/Test/Test',\n","    transform\n",")\n","\n","# DataLoaders\n","train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n","val_loader = DataLoader(val_data, batch_size=16, shuffle=False)\n","test_loader = DataLoader(test_data, batch_size=16, shuffle=False)\n","\n","print(\"Datasets and loaders prepared with updated transformations and test dataset.\")\n"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-11-20T04:10:03.472502Z","iopub.status.busy":"2024-11-20T04:10:03.472137Z","iopub.status.idle":"2024-11-20T04:20:03.834349Z","shell.execute_reply":"2024-11-20T04:20:03.833337Z","shell.execute_reply.started":"2024-11-20T04:10:03.472470Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting training with StepLR scheduler...\n","Epoch [1/50] - Training Loss: 0.1846 - Validation Loss: 1.0688\n","Best model saved!\n","Epoch [2/50] - Training Loss: 0.1353 - Validation Loss: 1.0711\n","Epoch [3/50] - Training Loss: 0.1311 - Validation Loss: 1.2100\n","Epoch [4/50] - Training Loss: 0.1617 - Validation Loss: 0.9917\n","Best model saved!\n","Epoch [5/50] - Training Loss: 0.1476 - Validation Loss: 1.1445\n","Epoch [6/50] - Training Loss: 0.1368 - Validation Loss: 1.0769\n","Epoch [7/50] - Training Loss: 0.1163 - Validation Loss: 1.4659\n","Epoch [8/50] - Training Loss: 0.1536 - Validation Loss: 1.0211\n","Epoch [9/50] - Training Loss: 0.0831 - Validation Loss: 1.2310\n","Epoch [10/50] - Training Loss: 0.0692 - Validation Loss: 1.3621\n","Epoch [11/50] - Training Loss: 0.1055 - Validation Loss: 1.7103\n","Epoch [12/50] - Training Loss: 0.1748 - Validation Loss: 1.4206\n","Epoch [13/50] - Training Loss: 0.0917 - Validation Loss: 1.2886\n","Epoch [14/50] - Training Loss: 0.0818 - Validation Loss: 1.4865\n","Early stopping triggered.\n","Training completed.\n"]}],"source":["import torch.optim as optim\n","\n","# Loss function\n","criterion = nn.CrossEntropyLoss()\n","\n","# Optimizer: AdamW for better generalization\n","optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=1e-3)\n","\n","# Scheduler: StepLR with step size of 10 epochs and gamma of 0.1\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","\n","def train_model(model, train_loader, val_loader, criterion, optimizer,  epochs=50, patience=10):\n","    best_val_loss = float(\"inf\")\n","    patience_counter = 0\n","\n","    print(\"Starting training with StepLR scheduler...\")\n","\n","    for epoch in range(epochs):\n","        # Training phase\n","        model.train()\n","        running_train_loss = 0.0\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            running_train_loss += loss.item()\n","\n","        avg_train_loss = running_train_loss / len(train_loader)\n","\n","        # Validation phase\n","        model.eval()\n","        running_val_loss = 0.0\n","        with torch.no_grad():\n","            for inputs, labels in val_loader:\n","                inputs, labels = inputs.to(device), labels.to(device)\n","                outputs = model(inputs)\n","                val_loss = criterion(outputs, labels)\n","                running_val_loss += val_loss.item()\n","\n","        avg_val_loss = running_val_loss / len(val_loader)\n","\n","        # Print training and validation loss\n","        print(f\"Epoch [{epoch+1}/{epochs}] - Training Loss: {avg_train_loss:.4f} - Validation Loss: {avg_val_loss:.4f}\")\n","\n","    \n","        # Save best model based on validation loss\n","        if avg_val_loss < best_val_loss:\n","            best_val_loss = avg_val_loss\n","            patience_counter = 0\n","            torch.save(model.state_dict(), \"best_model.pth\")\n","            print(\"Best model saved!\")\n","        else:\n","            patience_counter += 1\n","\n","        # Early stopping\n","        if patience_counter >= patience:\n","            print(\"Early stopping triggered.\")\n","            break\n","\n","    print(\"Training completed.\")\n","\n","# Train the model\n","train_model(model, train_loader, val_loader, criterion, optimizer, epochs=50, patience=10)\n"]},{"cell_type":"markdown","metadata":{},"source":["# 3. Evaluate your model using the developed software"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-11-20T04:28:36.837677Z","iopub.status.busy":"2024-11-20T04:28:36.837076Z","iopub.status.idle":"2024-11-20T04:28:46.188806Z","shell.execute_reply":"2024-11-20T04:28:46.187834Z","shell.execute_reply.started":"2024-11-20T04:28:36.837634Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Predictions saved to /kaggle/working/test_predictions.csv\n"]}],"source":["def generate_predictions(model, test_loader):\n","    model.eval()\n","    predictions = []\n","    with torch.no_grad():\n","        for images, filenames in test_loader:\n","            images = images.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs, 1)\n","            for i in range(len(filenames)):\n","                predictions.append((filenames[i], predicted[i].item()))\n","    return predictions\n","\n","# Generate and save predictions\n","predictions = generate_predictions(model, test_loader)\n","output_file = '/kaggle/working/test_predictions.csv'\n","pd.DataFrame(predictions, columns=[\"Filename\", \"Predicted Class\"]).to_csv(output_file, index=False, header=False)\n","print(f\"Predictions saved to {output_file}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# 4. Compare results with [RVT paper](https://www.nature.com/articles/s41598-023-50063-x). Requirement: performance is better than VGG16: 75%"]},{"cell_type":"markdown","metadata":{},"source":["The lightweight CNN model achieved a test accuracy of 71.75%, demonstrating its effectiveness in classifying canine heart X-rays into clinically relevant categories: Small, Normal, and Large. In comparison, VGG16, a widely used architecture in veterinary imaging, has been reported to achieve a test accuracy of approximately 75% in similar tasks. While the lightweight CNN does not surpass the performance of VGG16, it offers significant advantages in computational efficiency and model simplicity, making it more suitable for deployment in resource-constrained veterinary settings.\n","\n","The model's predictions were validated using the Dog X ray classification software.  The software evaluation highlighted the robustness of the model, particularly in distinguishing \"Normal\" and \"Large\" classes, where clinical accuracy is most critical for diagnosis."]},{"cell_type":"markdown","metadata":{},"source":["# 5. Write a four-page paper report using the shared LaTex template. Upload your paper to ResearchGate or Arxiv, and put your paper link and GitHub weight link here."]},{"cell_type":"markdown","metadata":{},"source":["https://www.researchgate.net/publication/385946967_AI-Powered_Cardiomegaly_Detection_in_Canine_X-rays_A_Lightweight_CNN_Approach"]},{"cell_type":"markdown","metadata":{},"source":["# 6. Grading rubric\n","\n","(1). Code ------- 20 points (you also need to upload your final model as a pt file)\n","\n","(2). Grammer ---- 20 points\n","\n","(3). Introduction & related work --- 10 points\n","\n","\n","(4). Method  ---- 20 points\n","\n","(5). Results ---- 20 points\n","\n","     > = 75 % -->10 points\n","     < 55 % -->0 points\n","     >= 55 % & < 75% --> 0.5 point/percent\n","     \n","\n","(6). Discussion - 10 points"]},{"cell_type":"markdown","metadata":{},"source":["![X-ray](./predict.png \"Result\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":6090241,"sourceId":9911773,"sourceType":"datasetVersion"},{"datasetId":6119679,"sourceId":9951075,"sourceType":"datasetVersion"}],"dockerImageVersionId":30786,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
